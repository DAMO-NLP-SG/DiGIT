# @package _group_

common:
  fp16: true
  log_format: json
  log_interval: 100
  min_loss_scale: 1e-6
  user_dir: fairseq_user

checkpoint:
  save_interval: 1
  save_interval_updates: 0
  keep_last_epochs: 1
  keep_interval_updates: -1
  no_epoch_checkpoints: true

task:
  _name: image_generation_stage1
  input_size: 256
  source_vocab_size: -1
  target_vocab_size: 8192
  augmentation: randresizedcrop
  linear_probe: true

dataset:
  train_subset: train
  valid_subset: val
  num_workers: 4
  batch_size: 256
  skip_invalid_size_inputs_valid_test: true
  required_batch_size_multiple: 1

criterion:
  _name: ce_loss_ls
  label_smoothing: 0
  report_accuracy: true

optimization:
  max_update: 100000
  update_freq: [1]

optimizer:
  _name: sgd_hydra
  momentum: 0.9

lr_scheduler:
  _name: cosine

model:
  _name: stlm_linearprobe
  kmeans_path: dataset/ILSVRC2012/dino_short_224_l3/km_8k.npy
  pretrained_enc_config:
    pretrained_enc_arch: dino_vit_base
    pretrained_enc_path: models/dinov2_vitb14_reg4_pretrain.pth
    pretrained_enc_proj_dim: 256
    pretrained_enc_withproj: True
    layer: 3
  decoder_embed_dim: 1024
  decoder_output_dim: 1024
  decoder_input_dim: 1024
  decoder_ffn_embed_dim: 4096
  decoder_layers: 16
  decoder_attention_heads: 16
  activation_fn: gelu
  decoder_learned_pos: true
  pretrained_decoder_path: outputs/dino_base_stage1/checkpoints/checkpoint_last.pt

